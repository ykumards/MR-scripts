For Hadoop-streaming, the mapper and reducer are executables which read from STDIN and write to STDOUT

The mapper should read from STDIN records, by default the line break is the record separator
The mapper should print to STDOUT key-value pairs, each line is one key-value pair, with the 
key and the value separated by a tab

The reducer should read (pre-sorted) key-value pairs from STDIN, again one per line with a
tab character separating key from value
The reducer should print a single key-value pair to STDOUT, also one per line, tab-delimited

You can test a Hadoop streaming pipeline on the bash shell, like:

./mapper < input_file | sort -k1,1 | ./reducer

NOTE: on Dumbo, 3rd-party software is installed as environment modules ('module avail' for a list). 
      However, MapReduce does not pass the environment to the mappers and reducers, so mappers and
      reducers must load the required modules themselves.
      To this end, the example mapper here uses 'numpy', which is avaiable via the module 'python/gnu/2.7.10'.
      To make this module availavble to the mapper, we wrap mapper.py in a shell script, mapper.sh, which 
      loads the relevent modules

To run this example under Hadoop-streaming, use:

mkdir -p $HOME/example
cd $HOME/example
cp -r /share/apps/examples/hadoop-streaming/src .
# put the input file onto hdfs:
hadoop fs -put /share/apps/examples/book.txt

# location of jar file for hadoop-streaming:
export HADOOP_HOME=/opt/cloudera/parcels/CDH-5.4.5-1.cdh5.4.5.p0.7/
# mapreduce will abort if the files it wants to write already exist, so first clean up from previous runs:
hadoop fs -rm -r example.out
hadoop jar $HADOOP_HOME/lib/hadoop-mapreduce/hadoop-streaming.jar -file `pwd`/src -mapper src/mapper.sh -reducer src/reducer.py -input book.txt -output example.out
# no example.parts has as many files as we had reducers (default is set on system).
# we can merge tham back to $HOME with:
hadoop fs -getmerge example.out
# .. or just use example.out (the resulting directory containing parts) as the input of another mapreduce job

# you can control the number of reducers with the argument "-D mapred.reduce.tasks=<number>", placed immediately after the hadoop-streaming jar, eg:
hadoop fs -rm -r example.out
hadoop jar $HADOOP_HOME/lib/hadoop-mapreduce/hadoop-streaming.jar -D mapred.reduce.tasks=9 -file `pwd`/src -mapper src/mapper.sh -reducer src/reducer.sh -input book.txt -output example.out
